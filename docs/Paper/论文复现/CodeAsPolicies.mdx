import WordCount from '../../../src/components/WordCount/WordCount';

<WordCount>


## 1. 摘要 

Abstract—Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings[1].We find that the secode-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions
or feedback loops that process perception outputs (e.g., from object
detectors [2], [3]) and parameterize control primitive APIs. When
provided as input several example language commands (formatted
as comments) followed by corresponding policy code (via few-shot
prompting), LLMs can take in new commands and autonomously
re-compose API calls to generate new policy code respectively. By
chaining classic logic structures and referencing third-party libraries
(e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way
can write robot policies that (i) exhibit spatial-geometric reasoning,
(ii) generalize to new instructions, and (iii) prescribe precise values
(e.g., velocities) to ambiguous descriptions (“faster”) depending
on context (i.e., behavioral commonsense). This paper presents
Code as Policies: a robot-centric formulation of language model
generated programs (LMPs) that can represent reactive policies (e.g.,
impedance controllers), as well as waypoint-based policies (vision-
based pick and place, trajectory-based control), demonstrated across
multiple real robot platforms. Central to our approach is prompting
hierarchical code-gen (recursively defining undefined functions),
which can write more complex code and also improves state-of-the-
art to solve 39.8% of problems on the HumanEval [1] benchmark.
Code and videos are available at https://code-as-policies.github.io
<details>

<summary>翻译</summary>

经过代码补全任务训练的大型语言模型（LLMs）已被证明能够根据文本注释生成简单的Python程序[1]。我们发现，这些擅长编写代码的LLM可通过重新定位用途，依据自然语言指令生成机器人策略代码。具体而言，策略代码可表述为处理感知输出（如来自目标检测器[2][3]）的函数或反馈回路，并参数化控制原语的应用程序编程接口（API）。当输入多个自然语言指令样例（以注释形式格式化）及其对应策略代码（通过少量示例提示）时，LLM能够接收新指令并自主重组API调用以生成相应的新策略代码。通过链式调用经典逻辑结构及引用第三方库（如NumPy、Shapely）执行算术运算，这种LLM可生成的机器人策略能够：(i) 展现空间几何推理能力；(ii) 泛化至新指令；(iii) 根据上下文（即行为常识）对模糊描述（如"更快"）赋予精确参数值（如速度值）。本文提出"代码即策略"（Code as Policies）框架——一种以机器人为核心的语言模型生成程序（LMPs）表述方式，能够表示反应式策略（如阻抗控制）和基于路径点的策略（视觉抓取放置、轨迹控制），并在多个真实机器人平台上完成验证。本方法的核心在于分层代码生成提示策略（通过递归定义未实现函数），该策略不仅能编写更复杂代码，还将HumanEval基准测试[1]的最优解决率提升至39.8%。

</details>

:::info

机器人策略（Robot Policy）指的是一套系统化的决策逻辑或行为规则，用于控制机器人如何根据感知到的情况（如传感器数据、视觉信息）生成具体的动作指令。

通俗来说，就是**机器人从“所感”导致“所想”，再到“所做”的这一层。**

摘要中指出，原先的研究证明了文本注释可以生成简单的Python程序，而现在该团队的进展是通过自然语言指令也可以生成策略代码。必须指出的是，文本注释和自然语言指令是有区别的，在下文中，

:::




## 2. 引言

为实现与物理世界的交互，依赖语言的机器人需将语言具象化（grounded）——即建立词汇、感知与动作之间的关联[4]。传统方法通过词法分析提取语义表征以指导策略（policies）[5]–[7]，但面对未见指令时表现受限。较新的端到端学习方法（语言直接映射到动作）[8]–[10]虽然能够提升泛化性，却需要大量真实机器人数据进行训练，成本高昂。

与此同时，自然语言处理领域的最新进展表明：通过互联网规模数据预训练的大型语言模型（LLMs） [11]–[13]具备直接应用于语言交互机器人的能力——例如，无需额外微调即可从自然语言指令中规划行为序列[16]–[18]]。这些行为序列可基于固定技能集合（通过行为克隆或强化学习预训练的既定策略[19]–[21]）在真实机器人功能可供性中通过价值函数建立关联。尽管此类方法前景广阔，但抽象分层（LLM仅规划技能调用）隔断了其对感知-动作反馈回路的直接影响，导致以下局限性：

1.难以泛化共享感知与动作的反馈模式
例如将“将苹果放在橙子上”扩展为“当检测到橙子时放下苹果”需重新设计逻辑。
2.无法表达控制中的常识性先验
如“更快移动”、“加大推力”需依赖隐含物理知识。
3.空间关系理解受限
如“将苹果稍微左移”需调用几何计算而非单纯路径规划。
因此，每增加一项新技能（或其关联的具象化模式）均需重新收集数据并整体训练——这虽将数据负担转移至技能获取阶段，但问题本质未变。这引出了核心问题：如何扩展LLMs的应用，使其超越单一技能序列规划？

我们的发现是：擅长代码生成的LLMs\[1\]\[11\]\[22\]可更进一步——能够统筹规划、策略逻辑与控制。代码补全训练的LLMs已可基于文本注释生成Python程序，而本研究证明其可被改造为按自然语言指令（注释形式）编写机器人策略代码。策略代码可定义函数或反馈循环以处理感知输出（如开放词汇物体检测器[2][3]结果）并参数化控制原语API（见图1）。当输入包含多个\<指令-代码\>示例（通过少量示例提示，灰色部分）后，LLM可针对新指令（绿色部分）自主重组API调用生成对应策略代码（高亮部分）：



## References

此处指的是调研过程中的参考文献。



</WordCount>
